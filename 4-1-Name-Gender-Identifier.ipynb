{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Gender Identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the last letter of the name to predict the gender. For instance, English names ending in “a”, “e”, and “i” are likely to be female, while those ending in “k”, “o”, “r”, “s”, and “t” are likely to be male.\n",
    "\n",
    "We start by building a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'a'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_gender_features_1(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "\n",
    "extract_gender_features_1('Samantha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the `names` corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names Corpus, Version 1.3 (1994-03-29)\n",
      "Copyright (C) 1991 Mark Kantrowitz\n",
      "Additions by Bill Ross\n",
      "\n",
      "This corpus contains 5001 female names and 2943 male names, sorted\n",
      "alphabetically, one per line.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "print(names.readme()[:195])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('female.txt')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a name gender classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare a list of examples and corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abagael', 'female'),\n",
       " ('Abagail', 'female'),\n",
       " ('Abbe', 'female'),\n",
       " ('Abbey', 'female'),\n",
       " ('Abbi', 'female')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_names = [(name, 'female') for name in names.words('female.txt')] + [\n",
    "    (name, 'male') for name in names.words('male.txt')\n",
    "]\n",
    "labeled_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by shuffling the data so that we can split it by index into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nelle', 'female'),\n",
       " ('Lorie', 'female'),\n",
       " ('Shelley', 'male'),\n",
       " ('Anna-Diana', 'female'),\n",
       " ('Corbin', 'male')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real research applications, consider using a seed value like `random.Random(4).shuffle()` instead, which ensures that your results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'y'}, 'male'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'male')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets_1 = [(extract_gender_features_1(n), gender) for (n, gender) in labeled_names]\n",
    "feature_sets_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_sets_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an 80–20 split into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "TRAIN_SET_SIZE = round(len(feature_sets_1) * TRAIN_TEST_SPLIT)\n",
    "train_set_1, test_set_1 = feature_sets_1[:TRAIN_SET_SIZE], feature_sets_1[TRAIN_SET_SIZE:]\n",
    "\n",
    "test_names = labeled_names[TRAIN_SET_SIZE:]  # For later use\n",
    "\n",
    "classifier_1 = NaiveBayesClassifier.train(train_set_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS:** When working with large corpora, constructing a list that contains the features of every instance, as we did above, can use a lot of memory. In these cases, it is better to use the `nltk.classify.apply_features()`, which returns an object that acts like a list but does not store all the feature sets in memory:\n",
    "\n",
    "```py\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "train_names = labeled_names[:round(len(feature_sets) * TRAIN_TEST_SPLIT)]\n",
    "test_names = labeled_names[round(len(feature_sets) * TRAIN_TEST_SPLIT):]\n",
    "\n",
    "train_set = apply_features(extract_gender_features_1, train_names)\n",
    "test_set = apply_features(extract_gender_features_1, test_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our classifier, we can print the likelihood ratios for the most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     35.1 : 1.0\n",
      "             last_letter = 'a'            female : male   =     34.2 : 1.0\n",
      "             last_letter = 'f'              male : female =     24.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     16.5 : 1.0\n",
      "             last_letter = 'd'              male : female =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_1.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female', 'male']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_1.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import accuracy\n",
    "\n",
    "round(accuracy(classifier_1, test_set_1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_1.classify(extract_gender_features_1('Aphrodite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_1.classify(extract_gender_features_1('Zeus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a classifier with more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try out a classifier with more features. We will now consider the first and last letter. Furthermore, we will also check whether each letter in the alphabet exists within the name and also examine, as a separate feature, the number of times it appears.\n",
    "\n",
    "While there is an overlap between the presence and count of letters in a name, it is wise to try both as features. It could turn out that the simpler presence feature or the more nuanced count feature can better predict gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE GENDER FEATURES FOR \"JOHN\"\n",
      "first_letter: j\n",
      "last_letter: n\n",
      "has(a): False\n",
      "count(a): 0\n"
     ]
    }
   ],
   "source": [
    "def extract_gender_features_2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"has({})\".format(letter)] = letter in name.lower()\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "    return features\n",
    "\n",
    "\n",
    "john_gender_features = extract_gender_features_2('John')\n",
    "print('EXAMPLE GENDER FEATURES FOR \"JOHN\"')\n",
    "print('first_letter:', john_gender_features['first_letter'])\n",
    "print('last_letter:', john_gender_features['last_letter'])\n",
    "print('has(a):', john_gender_features['has(a)'])\n",
    "print('count(a):', john_gender_features['count(a)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets_2 = [(extract_gender_features_2(n), gender) for (n, gender) in labeled_names]\n",
    "train_set_2, test_set_2 = feature_sets_2[:TRAIN_SET_SIZE], feature_sets_2[TRAIN_SET_SIZE:]\n",
    "\n",
    "classifier_2 = NaiveBayesClassifier.train(train_set_2)\n",
    "round(accuracy(classifier_2, test_set_2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have expected that having too many specific features on a small dataset would lead to overfitting. But it seems the classifier was able to avoid that, as its accuracy is comparable to the previous one’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     35.1 : 1.0\n",
      "             last_letter = 'a'            female : male   =     34.2 : 1.0\n",
      "             last_letter = 'f'              male : female =     24.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     16.5 : 1.0\n",
      "             last_letter = 'd'              male : female =     11.7 : 1.0\n",
      "             last_letter = 'z'              male : female =      9.7 : 1.0\n",
      "             last_letter = 'v'              male : female =      8.5 : 1.0\n",
      "                count(v) = 2              female : male   =      8.4 : 1.0\n",
      "             last_letter = 'm'              male : female =      7.7 : 1.0\n",
      "             last_letter = 'o'              male : female =      6.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the classifier is mainly using the last letter feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing the two classifiers using `nltk.metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, here’s a useful function for comparing strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "edit_distance(\"John\", \"Joan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.metrics` module provides functions for calculating metrics beyond just accuracy. To use it, we need to build two sets for each classification label: a reference set of correct values and a test set of observed values.\n",
    "\n",
    "Starting with the reference sets, we will first build a dictionary with two keys, `male` and `female`. The value for each key will be a set containing all the indices of male and female names, respectively. As for the test sets, we will build a similar dictionary for each classifier, where, this time, the values for the `male` and `female` keys will be the set of indices that the classifier predicted to be male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "ref_sets = collections.defaultdict(set)\n",
    "test_sets_1 = collections.defaultdict(set)\n",
    "test_sets_2 = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set_1):\n",
    "    ref_sets[label].add(i)\n",
    "\n",
    "    observed_1 = classifier_1.classify(feats)\n",
    "    test_sets_1[observed_1].add(i)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set_2):\n",
    "    observed_2 = classifier_2.classify(feats)\n",
    "    test_sets_2[observed_2].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to print the metrics for each classifier. This will include all metrics except for the accuracy and the confusion matrix. We actually cannot obtain the accuracy in this manner because `nltk.metrics.scores.accuracy(reference, test)` works by comparing `test[i] == reference[i]`, and our reference and test data are not formatted in a way that allows for this. `nltk.metrics.confusionmatrix.ConfusionMatrix(reference, test)` also works the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASSIFIER 1\n",
      "------------\n",
      "Female precision: 0.8\n",
      "Male precision: 0.68\n",
      "Female recall: 0.82\n",
      "Male recall: 0.65\n",
      "Female F1 score: 0.81\n",
      "Male F1 score: 0.67\n",
      "\n",
      "CLASSIFIER 2\n",
      "------------\n",
      "Female precision: 0.81\n",
      "Male precision: 0.69\n",
      "Female recall: 0.82\n",
      "Male recall: 0.67\n",
      "Female F1 score: 0.81\n",
      "Male F1 score: 0.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "\n",
    "args1 = (\n",
    "    round(precision(ref_sets['female'], test_sets_1['female']), 2),\n",
    "    round(precision(ref_sets['male'], test_sets_1['male']), 2),\n",
    "    round(recall(ref_sets['female'], test_sets_1['female']), 2),\n",
    "    round(recall(ref_sets['male'], test_sets_1['male']), 2),\n",
    "    round(f_measure(ref_sets['female'], test_sets_1['female']), 2),\n",
    "    round(f_measure(ref_sets['male'], test_sets_1['male']), 2),\n",
    ")\n",
    "\n",
    "args2 = (\n",
    "    round(precision(ref_sets['female'], test_sets_2['female']), 2),\n",
    "    round(precision(ref_sets['male'], test_sets_2['male']), 2),\n",
    "    round(recall(ref_sets['female'], test_sets_2['female']), 2),\n",
    "    round(recall(ref_sets['male'], test_sets_2['male']), 2),\n",
    "    round(f_measure(ref_sets['female'], test_sets_2['female']), 2),\n",
    "    round(f_measure(ref_sets['male'], test_sets_2['male']), 2),\n",
    ")\n",
    "\n",
    "print(\n",
    "    '''\n",
    "CLASSIFIER 1\n",
    "------------\n",
    "Female precision: {0}\n",
    "Male precision: {1}\n",
    "Female recall: {2}\n",
    "Male recall: {3}\n",
    "Female F1 score: {4}\n",
    "Male F1 score: {5}\n",
    "\n",
    "CLASSIFIER 2\n",
    "------------\n",
    "Female precision: {6}\n",
    "Male precision: {7}\n",
    "Female recall: {8}\n",
    "Male recall: {9}\n",
    "Female F1 score: {10}\n",
    "Male F1 score: {11}\n",
    "'''.format(\n",
    "        *args1, *args2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look for patterns in the classification errors made by the second classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 'male', 'Windy'),\n",
       " ('female', 'male', 'Joey'),\n",
       " ('male', 'female', 'Israel'),\n",
       " ('male', 'female', 'Dean'),\n",
       " ('female', 'male', 'Sheree')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = []\n",
    "for name, tag in test_names:\n",
    "    guess = classifier_2.classify(extract_gender_features_2(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))\n",
    "\n",
    "errors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s sort the errors and print a few of them in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct = female   guess = male     name = Aubry\n",
      "Correct = female   guess = male     name = Audrey\n",
      "Correct = female   guess = male     name = Audry\n",
      "Correct = female   guess = male     name = Bab\n",
      "Correct = female   guess = male     name = Bell\n",
      "Correct = female   guess = male     name = Beret\n",
      "Correct = female   guess = male     name = Bert\n",
      "Correct = female   guess = male     name = Berte\n",
      "Correct = female   guess = male     name = Berty\n",
      "Correct = female   guess = male     name = Beryl\n"
     ]
    }
   ],
   "source": [
    "for tag, guess, name in sorted(errors)[:10]:\n",
    "    print('Correct = {:8} guess = {:8} name = {}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go through the whole list of errors, we can observe that suffixes that are more than one letter long can be indicative of name genders. E.g., names ending in “ch” appear to be predominantly male, even though names that end in “n” tend to be female; and names ending in “yn” are predominantly female, even though names that end in “h” tend to be male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building a classifier with even more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE GENDER FEATURES FOR \"JACQUELINE\"\n",
      "first_letter: j\n",
      "suffix_1: e\n",
      "suffix_2: ne\n",
      "has(a): True\n",
      "count(a): 1\n"
     ]
    }
   ],
   "source": [
    "def extract_gender_features_3(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"suffix_1\"] = name[-1].lower()\n",
    "    features[\"suffix_2\"] = name[-2:].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"has({})\".format(letter)] = letter in name.lower()\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "    return features\n",
    "\n",
    "\n",
    "jacqueline_gender_features = extract_gender_features_3('Jacqueline')\n",
    "\n",
    "print('EXAMPLE GENDER FEATURES FOR \"JACQUELINE\"')\n",
    "print('first_letter:', jacqueline_gender_features['first_letter'])\n",
    "print('suffix_1:', jacqueline_gender_features['suffix_1'])\n",
    "print('suffix_2:', jacqueline_gender_features['suffix_2'])\n",
    "print('has(a):', jacqueline_gender_features['has(a)'])\n",
    "print('count(a):', jacqueline_gender_features['count(a)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets_3 = [(extract_gender_features_3(n), gender) for (n, gender) in labeled_names]\n",
    "train_set_3, test_set_3 = feature_sets_3[:TRAIN_SET_SIZE], feature_sets_3[TRAIN_SET_SIZE:]\n",
    "\n",
    "classifier_3 = NaiveBayesClassifier.train(train_set_3)\n",
    "round(accuracy(classifier_3, test_set_3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                suffix_2 = 'na'           female : male   =     85.8 : 1.0\n",
      "                suffix_2 = 'la'           female : male   =     64.4 : 1.0\n",
      "                suffix_2 = 'rt'             male : female =     46.2 : 1.0\n",
      "                suffix_2 = 'rd'             male : female =     41.8 : 1.0\n",
      "                suffix_2 = 'ta'           female : male   =     37.7 : 1.0\n",
      "                suffix_1 = 'k'              male : female =     35.1 : 1.0\n",
      "                suffix_1 = 'a'            female : male   =     34.2 : 1.0\n",
      "                suffix_2 = 'ia'           female : male   =     32.9 : 1.0\n",
      "                suffix_2 = 'ra'           female : male   =     32.4 : 1.0\n",
      "                suffix_2 = 'us'             male : female =     26.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_3.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trying a maximum entropy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle of **maximum entropy** states that the probability distribution that best represents the current state of knowledge is the one with the largest entropy.\n",
    "\n",
    "The principle of maximum entropy is invoked when we have some information about a probability distribution but not enough to characterize it completely—likely because we do not have the means or resources to do so. For example, if all we know about a distribution is its average, we can imagine infinite shapes that yield a particular average. The principle of maximum entropy says that we should humbly opt for the distribution that maximizes the unpredictability contained in the distribution.\n",
    "\n",
    "Taking the idea to the extreme, it wouldn’t be scientific to choose a distribution that yields the average value 100% of the time.\n",
    "\n",
    "From all the models that fit our training data, the maximum entropy classifier selects the one with the largest entropy. Due to the minimum assumptions that the maximum entropy classifier makes, it is usually used when we don’t know anything about the prior distributions and when it is unsafe to make any assumptions. The maximum entropy classifier is also used when we can’t assume the conditional independence of the features.\n",
    "\n",
    "NLTK’s `MaxentClassifier.train()` method takes an argument called `max_iter` with a default value of 100. This would take a long time to run. In this example, the performance in terms of accuracy on the test set starts significantly improving beyond the previous model’s at around 15 iterations, so we will set `max_iter` to `20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (20 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.369\n",
      "             2          -0.60103        0.631\n",
      "             3          -0.57930        0.631\n",
      "             4          -0.55934        0.639\n",
      "             5          -0.54105        0.670\n",
      "             6          -0.52434        0.705\n",
      "             7          -0.50908        0.736\n",
      "             8          -0.49516        0.757\n",
      "             9          -0.48244        0.767\n",
      "            10          -0.47081        0.780\n",
      "            11          -0.46016        0.789\n",
      "            12          -0.45039        0.794\n",
      "            13          -0.44141        0.797\n",
      "            14          -0.43315        0.802\n",
      "            15          -0.42552        0.804\n",
      "            16          -0.41847        0.806\n",
      "            17          -0.41193        0.807\n",
      "            18          -0.40587        0.808\n",
      "            19          -0.40023        0.811\n",
      "         Final          -0.39498        0.812\n"
     ]
    }
   ],
   "source": [
    "from nltk import MaxentClassifier\n",
    "\n",
    "me_classifier = MaxentClassifier.train(train_set_3, max_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies printed above were on the training set, so now we will check the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(accuracy(me_classifier, test_set_3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -1.642 suffix_2=='na' and label is 'male'\n",
      "  -1.603 suffix_2=='la' and label is 'male'\n",
      "  -1.343 suffix_2=='rt' and label is 'female'\n",
      "  -1.315 suffix_2=='ta' and label is 'male'\n",
      "  -1.281 suffix_2=='ra' and label is 'male'\n",
      "  -1.222 suffix_1=='a' and label is 'male'\n",
      "  -1.198 suffix_2=='ia' and label is 'male'\n",
      "  -1.196 suffix_1=='k' and label is 'female'\n",
      "  -1.186 suffix_2=='rd' and label is 'female'\n",
      "  -1.089 suffix_1=='f' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. More classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` (sklearn) is a popular library that features various classification, regression, and clustering algorithms, including support vector machines, random forests, gradient boosting, k-means, and DBSCAN.\n",
    "\n",
    "NLTK provides a wrapper around sklearn classifiers, `nltk.classify.scikitlearn`, which is useful for quick experiments. The other option is to import and use sklearn directly.\n",
    "\n",
    "For an example of integrating sklearn with NLTK, you can check out Liling Tan’s [Basic NLP with NLTK](https://www.kaggle.com/code/alvations/basic-nlp-with-nltk) notebook on Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
