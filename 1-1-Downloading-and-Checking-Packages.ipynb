{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Checking Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you install and can successfully import all packages below without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # https://www.nltk.org/index.html\n",
    "import numpy  # https://numpy.org/\n",
    "import matplotlib.pyplot  # https://matplotlib.org/stable/\n",
    "import unidecode  # https://pypi.org/project/Unidecode/\n",
    "import langdetect  # https://pypi.org/project/langdetect/\n",
    "import langid  # https://github.com/saffsd/langid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes with many corpora, toy grammars, trained models, and tools. We need to download the NLTK data required for this tutorial before proceeding:\n",
    "\n",
    "1. `popular`: Collection of “popular” packages. See the list of included packages in [this Stack Overflow answer](https://stackoverflow.com/a/30822962/4304516).\n",
    "2. `punkt_tab`: Tokenizer for languages that use tabs as word separators (e.g., Chinese, Japanese, and Thai). While we will not be working with these languages, the tokenizers we will use require this package.\n",
    "3. `maxent_ne_chunker_tab`: Named entity chunker for languages that use tabs as word separators\n",
    "4. `averaged_perceptron_tagger_eng`: English part-of-speech tagger based on the averaged perceptron algorithm\n",
    "5. `tagsets_json`: JSON files for NLTK’s built-in tagsets\n",
    "6. `vader_lexicon`: Lexicon used by the VADER sentiment analysis tool\n",
    "7. `brown`: The first million-word electronic corpus of English, created in 1961 at Brown University. It contains text from 500 sources categorized by genre.\n",
    "8. `reuters`: Corpus with news articles classified under different categories\n",
    "9. `subjectivity`: A dataset containing 5,000 subjective and 5,000 objective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets_json')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('subjectivity')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
