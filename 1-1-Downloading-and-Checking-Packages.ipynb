{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Checking Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you install and can successfully import all packages below without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # https://www.nltk.org/install.html\n",
    "import numpy # https://www.scipy.org/install.html\n",
    "import matplotlib.pyplot # https://matplotlib.org/downloads.html\n",
    "import tweepy # https://github.com/tweepy/tweepy\n",
    "import TwitterSearch # https://github.com/ckoepp/TwitterSearch\n",
    "import unidecode # https://pypi.python.org/pypi/Unidecode\n",
    "import langdetect # https://pypi.python.org/pypi/langdetect\n",
    "import langid # https://github.com/saffsd/langid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK comes with many corpora, toy grammars, trained models, etc. We need to download the NLTK data required for this tutorial before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular') # Collection of \"popular\" packages. See the list of included packages here: https://stackoverflow.com/a/30822962/4304516.\n",
    "\n",
    "nltk.download('punkt_tab') # Tokenizer for languages that use tabs as word separators (e.g., Chinese, Japanese, Thai). Even though we will not be working with these languages, the tokenizer we will use requires this package.\n",
    "nltk.download('maxent_ne_chunker_tab') # Named entity chunker for languages that use tabs as word separators\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng') # English part-of-speech tagger based on the averaged perceptron algorithm\n",
    "nltk.download('tagsets_json') # JSON files for NLTK's built-in tagsets\n",
    "nltk.download('vader_lexicon') # Lexicon used by the VADER sentiment analysis tool\n",
    "\n",
    "nltk.download('brown') # The first million-word electronic corpus of English, created in 1961 at Brown University. It contains text from 500 sources categorized by genre.\n",
    "nltk.download('reuters') # Corpus with news articles classified under different categories\n",
    "nltk.download('subjectivity') # Dataset containing 5,000 subjective and 5,000 objective sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
