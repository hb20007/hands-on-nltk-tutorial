{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK with Non-Latin Scripts (Greek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to work with non-Latin scripts, taking Greek as an example.\n",
    "\n",
    "You probably know that all characters, such as letters, are represented as numerical values by a computer. This is called **character encoding**. When computers were first created, there was no standard for the numerical values for different characters. To solve this problem, the American Standards Association created the **ASCII** encoding in the 1960s, which standardized the representation of English characters.\n",
    "\n",
    "In the following years, as more countries grew and modernized, the need for a standard encoding beyond English characters became clear. In the 1990s, the International Organization for Standardization (ISO) developed the **Unicode** character encoding system to enable universal support for all languages. At the abstract level, Unicode assigns a number called a **code point** to every character in every writing system.\n",
    "\n",
    "As we will be working in a completely non-Latin script, we will be working with Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'αυτος είναι ο χορός της βροχής της φυλής· ό,τι περίεργο.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"ΑΥΤΟΣ είναι ο χορός της βροχής της φυλής· ό,τι περίεργο.\"\n",
    "sentence = sentence.lower()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A package called [`unidecode`](https://pypi.org/project/Unidecode) can be used to transliterate any Unicode string into the “closest possible representation” in ASCII text. This can sometimes be useful for certain applications when working with non-Latin scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autos einai o khoros tes brokhes tes phules* o,ti periergo.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "sentence_latin = unidecode(sentence)\n",
    "sentence_latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with removing the accents from the letters. First, we need to learn how putting accents on letters actually works in Unicode. Accents are **combining diacritical marks** that can be added after the base character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. For example, `é` can be represented as `U+0065 e LATIN SMALL LETTER E` followed by `U+0301 ◌́ COMBINING ACUTE ACCENT`, or as the precomposed character `U+00E9 é LATIN SMALL LETTER E WITH ACUTE`. The mechanism of **canonical equivalence** within *The Unicode Standard* ensures that both representations are equivalent.\n",
    "\n",
    "The standard also defines a normalization procedure, called **Unicode normalization**, where equivalent sequences of characters are replaced so that any two texts that are equivalent will be reduced to the same sequence of code points. There are four **normal forms** that we can choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the accents from our text, we can first convert it to **Normalization Form Canonical Decomposition (NFD)**, one of the four Unicode normal forms. In this form, all combining characters are separated from the base letters. Then, we can take advantage of the fact that each Unicode code point has a **General Category** property which identifies what kind of character it is. Accents have the `\"Mn\"` category, which stands for “Mark, nonspacing”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'αυτος ειναι ο χορος της βροχης της φυλης· ο,τι περιεργο.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "sentence_no_accents = strip_accents(sentence)\n",
    "sentence_no_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['αυτος',\n",
       " 'ειναι',\n",
       " 'ο',\n",
       " 'χορος',\n",
       " 'της',\n",
       " 'βροχης',\n",
       " 'της',\n",
       " 'φυλης·',\n",
       " 'ο,τι',\n",
       " 'περιεργο.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokens = WhitespaceTokenizer().tokenize(sentence_no_accents)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized the sentence, let’s remove the punctuation. Specifically, we need to remove `·`, which is the Greek equivalent of a semicolon.\n",
    "\n",
    "We should not remove the `,` in the `ο,τι` token. It is actually part of the word. This is a separation mark called a “hypodiastole” that was used in Ancient Greek to distinguish words or phrases from others that are spelled similarly. In Modern Greek, this is represented by a comma and it only survived in this particular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s `string.punctuation` only includes ASCII punctuation characters. [This Stack Overflow question](https://stackoverflow.com/questions/60983836/complete-set-of-punctuation-marks-for-python-not-just-ascii) provides suggestions for obtaining a complete set of Unicode punctuation marks. However, for the purposes of this notebook, we will just append the list of Greek punctuation marks that do not appear in `string.punctuation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–—…“”‘’«»·'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_extended = punctuation + '–—…“”‘’«»·'\n",
    "punctuation_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['αυτος',\n",
       " 'ειναι',\n",
       " 'ο',\n",
       " 'χορος',\n",
       " 'της',\n",
       " 'βροχης',\n",
       " 'της',\n",
       " 'φυλης',\n",
       " 'ο,τι',\n",
       " 'περιεργο']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token == 'ο,τι':\n",
    "        clean_tokens.append('ο,τι')\n",
    "    else:\n",
    "        clean_tokens.append(\n",
    "            token.translate(str.maketrans(dict.fromkeys(punctuation_extended, None)))\n",
    "        )\n",
    "\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s filter out stopwords. We will use a list of Greek stopwords adapted from [6/stopwords-json](https://github.com/6/stopwords-json), which contains stopword lists for 50 languages. However, better lists with more stopwords are available, like [Dr. Holger Bagola’s Greek stopword list](https://www.translatum.gr/forum/index.php?topic=3550.0?topic=3550.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greek_stopwords = [\n",
    "    'αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτες', 'αυτη', 'αυτο', 'αυτοι', 'αυτος',\n",
    "    'αυτους', 'αυτων', 'για', 'δε', 'δεν', 'εαν', 'ειμαι', 'ειμαστε', 'ειναι',\n",
    "    'εισαι', 'ειστε', 'εκεινα', 'εκεινες', 'εκεινη', 'εκεινο', 'εκεινοι',\n",
    "    'εκεινος', 'εκεινους', 'εκεινων', 'ενω', 'επι', 'η', 'θα', 'ισως', 'κ', 'και',\n",
    "    'κατα', 'κι', 'μα', 'με', 'μετα', 'μη', 'μην', 'να', 'ο', 'οι', 'ομως', 'οπως',\n",
    "    'οσο', 'οτι', 'ο,τι', 'παρα', 'ποια', 'ποιες', 'ποιο', 'ποιοι', 'ποιος',\n",
    "    'ποιους', 'ποιων', 'που', 'προς', 'πως', 'σε', 'στη', 'στην', 'στο', 'στον',\n",
    "    'στης', 'στου', 'στους', 'στις', 'στα', 'τα', 'την', 'της', 'το', 'τον',\n",
    "    'τοτε', 'του', 'των', 'τις', 'τους', 'ως',\n",
    "]\n",
    "len(greek_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['χορος', 'βροχης', 'φυλης', 'περιεργο']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens_set = set(clean_tokens)\n",
    "greek_stopwords_set = set(greek_stopwords)\n",
    "intersection_set = clean_tokens_set.intersection(greek_stopwords_set)\n",
    "\n",
    "for element in intersection_set:\n",
    "    clean_tokens = list(filter((element).__ne__, clean_tokens))\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more interesting packages like [`polyglot`](https://pypi.org/project/polyglot/) and [`greek-stemmer`](https://pypi.org/project/greek-stemmer/). These require [`PyICU`](https://pypi.org/project/PyICU/) to be installed first. `PyICU` is a Python extension wrapping the International Components for Unicode C++ libraries that implement much of the Unicode Standard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
