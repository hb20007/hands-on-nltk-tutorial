{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a PoS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the corpora included with NLTK have been **tagged** for their parts-of-speech. These are represented as tagged tokens—tuples consisting of each token (word) and its corresponding tag (part of speech). In this notebook, we will be using `brown`, which is a tagged corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a classifier that uses word endings to classify words into their parts of speech. First, we need to work out which suffixes are most informative for PoS tagging. We can begin by simply finding out what the most common suffixes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e': 202946, ',': 175002, '.': 152999, 's': 128722, 'd': 105687, 't': 94459, 'he': 92084, 'n': 87889, 'a': 74912, 'of': 72978, ...})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "\n",
    "suffix_f_dist = FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_f_dist[word[-1:]] += 1\n",
    "    suffix_f_dist[word[-2:]] += 1\n",
    "    suffix_f_dist[word[-3:]] += 1\n",
    "\n",
    "suffix_f_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, _) in suffix_f_dist.most_common(100)]\n",
    "common_suffixes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll define a feature extractor function which checks a given word for these suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS SUFFIX FEATURES FOR THE WORD \"TEST\"\n",
      "endswith(e): False\n",
      "endswith(t): True\n"
     ]
    }
   ],
   "source": [
    "def pos_suffix_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "\n",
    "test_features = pos_suffix_features('test')\n",
    "\n",
    "print('POS SUFFIX FEATURES FOR THE WORD \"TEST\"')\n",
    "print('endswith(e):', test_features['endswith(e)'])\n",
    "print('endswith(t):', test_features['endswith(t)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve defined our feature extractor, we can use it to train a decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the `tagged_words()` method to read the `brown` corpus as a list of tagged tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract all suffix features for each word in the list of tagged tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS SUFFIX FEATURES FOR THE WORD \"THE\"\n",
      "endswith(e): True\n",
      "endswith(t): False\n"
     ]
    }
   ],
   "source": [
    "feature_sets = [(pos_suffix_features(n), g) for (n, g) in tagged_words]\n",
    "\n",
    "print('POS SUFFIX FEATURES FOR THE WORD \"THE\"')\n",
    "print('endswith(e):', feature_sets[0][0]['endswith(e)'])\n",
    "print('endswith(t):', feature_sets[0][0]['endswith(t)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import DecisionTreeClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "cutoff = int(len(feature_sets) * 0.1)\n",
    "train_set, test_set = feature_sets[cutoff:], feature_sets[:cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a learning toolkit not optimized for speed. The next line of code, which trains the classifier, can take up to 10 minutes to execute, depending on the processor. For a practical classifier, use a library like `scikit-learn` instead of the classifiers that come with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(accuracy(classifier, test_set), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_suffix_features('goblins'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of decision tree models is that they are often easy to interpret. We can instruct NLTK to print them out as pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(the) == False: \n",
      "  if endswith(,) == False: \n",
      "    if endswith(s) == False: \n",
      "      if endswith(.) == False: return '.'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(s) == True: \n",
      "      if endswith(is) == False: return 'PP$'\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "  if endswith(,) == True: return ','\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the classifier, we can add contextual features:\n",
    "\n",
    "```py\n",
    "def pos_features(sentence, i): [1]\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "```\n",
    "\n",
    "Then, instead of working with tagged words, we work with tagged sentences:\n",
    "\n",
    "```py\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "```\n",
    "\n",
    "We can then improve this further by adding more features such as `prev-tag`, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
