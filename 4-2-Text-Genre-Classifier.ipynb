{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Genre Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on *Another Exercise: Classifying News Documents in Categories: sport, humor, adventure, science fiction, etc.* in [Natural Language Processing with Python/NLTK](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb) by Luciano M. Guasco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the `brown` corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus consists of 500 samples, distributed across 15 genres. Each sample begins at a random sentence-boundary in the article or other unit chosen, and continues up to the first sentence boundary after 2,000 words.\n",
    "\n",
    "1. **PRESS: Reportage** *(44 texts)*\n",
    "2. **PRESS: Editorial** *(27 texts)*\n",
    "3. **PRESS: Reviews** *(17 texts)*\n",
    "4. **RELIGION** *(17 texts)*\n",
    "5. **SKILL AND HOBBIES** *(36 texts)*\n",
    "6. **POPULAR LORE** *(48 texts)*\n",
    "7. **BELLES-LETTRES** – Biography, Memoirs, etc. *(75 texts)*\n",
    "8. **MISCELLANEOUS: US Government & House Organs** *(30 texts)*\n",
    "9. **LEARNED** – Natural sciences, Medicine, Mathematics, etc. *(80 texts)*\n",
    "10. **FICTION: General** *(29 texts)*\n",
    "11. **FICTION: Mystery and Detective Fiction** *(24 texts)*\n",
    "12. **FICTION: Science** *(6 texts)*\n",
    "13. **FICTION: Adventure and Western** *(29 texts)*\n",
    "14. **FICTION: Romance and Love Story** *(29 texts)*\n",
    "15. **HUMOR** *(9 texts)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown.readme()[:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01', 'ca02', 'ca03', 'ca04', 'ca05']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents('ca01')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compiling a list of the most popular words in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will supply all the words in the corpus to `FreqDist` to count the number of times each word appears. We will use Python strings’ `isalpha()` method to make sure that punctuation tokens are not included, even though this will also exclude words like `1` (very common), `Aug.`, `1913`, `13th`, `over-all`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 69971, 'of': 36412, 'and': 28853, 'to': 26158, 'a': 23195, 'in': 21337, 'that': 10594, 'is': 10109, 'was': 9815, 'he': 9548, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "word_freq_in_corpus = FreqDist(w.lower() for w in brown.words() if w.isalpha())\n",
    "word_freq_in_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert `word_freq_in_corpus` to a list of lists to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 69971],\n",
       " ['fulton', 17],\n",
       " ['county', 155],\n",
       " ['grand', 48],\n",
       " ['jury', 67],\n",
       " ['said', 1961],\n",
       " ['friday', 60],\n",
       " ['an', 3740],\n",
       " ['investigation', 51],\n",
       " ['of', 36412]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_in_corpus = list(map(list, word_freq_in_corpus.items()))\n",
    "word_freq_in_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 69971],\n",
       " ['of', 36412],\n",
       " ['and', 28853],\n",
       " ['to', 26158],\n",
       " ['a', 23195],\n",
       " ['in', 21337],\n",
       " ['that', 10594],\n",
       " ['is', 10109],\n",
       " ['was', 9815],\n",
       " ['he', 9548]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_in_corpus_sorted = sorted(word_freq_in_corpus, key=lambda x: x[1], reverse=True)\n",
    "word_freq_in_corpus_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the'],\n",
       " ['of'],\n",
       " ['and'],\n",
       " ['to'],\n",
       " ['a'],\n",
       " ['in'],\n",
       " ['that'],\n",
       " ['is'],\n",
       " ['was'],\n",
       " ['he']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1500_words = word_freq_in_corpus_sorted[:1500]\n",
    "\n",
    "for list_item in top_1500_words:\n",
    "    del list_item[1]\n",
    "\n",
    "top_1500_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to flatten `top_1500_words`. We can do this by breaking down the list into its individual sublists and then chaining them with `itertools.chain`. This returns a value of type `itertools.chain` that we can then cast to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', 'was', 'he']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "chain = itertools.chain(*top_1500_words)\n",
    "top_1500_words = list(chain)\n",
    "top_1500_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s remove stop words from our top 1500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'would', 'said', 'new', 'could', 'time', 'two', 'may', 'first', 'like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def filter_out_stopwords(words_list):\n",
    "    return [word for word in words_list if word not in stop_words]\n",
    "\n",
    "\n",
    "top_1500_words_filtered = filter_out_stopwords(top_1500_words)\n",
    "top_1500_words_filtered[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting the corpus to a form suitable for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file in the corpus ultimately needs to be represented by a dictionary indicating the presence of the corpus’s most popular words in the particular file.\n",
    "\n",
    "We will start by representing the corpus as a list of tuples, where each tuple contains a list of all the words in a file of the corpus and the category of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dan', 'morgan', 'told', 'would', 'forget', 'ann', 'turner', 'he', 'well', 'rid'] adventure\n"
     ]
    }
   ],
   "source": [
    "words_and_categories_by_file = [\n",
    "    (\n",
    "        [item.lower() for item in filter_out_stopwords(brown.words(fileid)) if item.isalpha()],\n",
    "        category,\n",
    "    )\n",
    "    for category in brown.categories()\n",
    "    for fileid in brown.fileids(category)\n",
    "]\n",
    "print(words_and_categories_by_file[0][0][:10], words_and_categories_by_file[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thomas', 'douglas', 'fifth', 'earl', 'selkirk', 'noble', 'humanitarian', 'scot', 'concerned', 'plight'] lore\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(words_and_categories_by_file)\n",
    "print(words_and_categories_by_file[0][0][:10], words_and_categories_by_file[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract features that indicate the presence or not of the top 1500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOME EXAMPLE WORD PRESENCE AND ABSENCE FEATURES FOR THE FIRST CORPUS FILE\n",
      "has(one): True\n",
      "has(would): True\n",
      "has(said): False\n",
      "has(new): True\n",
      "has(time): True\n"
     ]
    }
   ],
   "source": [
    "def extract_word_presence_absence_features(word_list):\n",
    "    words_set = set(word_list)\n",
    "    features_dict = {}\n",
    "    for word in top_1500_words_filtered:\n",
    "        features_dict['has(%s)' % word] = word in words_set\n",
    "    return features_dict\n",
    "\n",
    "\n",
    "features_and_categories_by_file = [\n",
    "    (extract_word_presence_absence_features(d), c) for (d, c) in words_and_categories_by_file\n",
    "]\n",
    "\n",
    "print('SOME EXAMPLE WORD PRESENCE AND ABSENCE FEATURES FOR THE FIRST CORPUS FILE')\n",
    "print('has(one):', features_and_categories_by_file[0][0]['has(one)'])\n",
    "print('has(would):', features_and_categories_by_file[0][0]['has(would)'])\n",
    "print('has(said):', features_and_categories_by_file[0][0]['has(said)'])\n",
    "print('has(new):', features_and_categories_by_file[0][0]['has(new)'])\n",
    "print('has(time):', features_and_categories_by_file[0][0]['has(time)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               has(wife) = True            humor : learne =     33.6 : 1.0\n",
      "              has(music) = True           review : learne =     29.7 : 1.0\n",
      "               has(dark) = True           advent : learne =     29.7 : 1.0\n",
      "              has(woman) = True           fictio : learne =     28.4 : 1.0\n",
      "                has(god) = True           religi : learne =     28.0 : 1.0\n",
      "          has(telephone) = True            humor : belles =     26.1 : 1.0\n",
      "            has(watched) = True           advent : learne =     26.1 : 1.0\n",
      "            has(waiting) = True           myster : learne =     25.0 : 1.0\n",
      "             has(walked) = True           myster : learne =     25.0 : 1.0\n",
      "            has(playing) = True           review : learne =     24.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "TRAIN_SET_SIZE = round(len(features_and_categories_by_file) * TRAIN_TEST_SPLIT)\n",
    "train_set = features_and_categories_by_file[:TRAIN_SET_SIZE]\n",
    "test_set = features_and_categories_by_file[TRAIN_SET_SIZE:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import accuracy\n",
    "\n",
    "round(accuracy(classifier, test_set), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to classify the `ca01` file, which is under the “news” category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(extract_word_presence_absence_features(brown.words('ca01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try classifying our own text. It needs to be long enough to contain a significant number of the top 1500 words, and needs to belong to a clear category.\n",
    "\n",
    "We will use the first section of the prologue of the Catechism of the Catholic Church, which should be classified under “religion”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "catechism_text = '''1 God, infinitely perfect and blessed in himself, in a plan of sheer goodness\n",
    "                    freely created man to make him share in his own blessed life. For this reason,\n",
    "                    at every time and in every place, God draws close to man. He calls man to seek\n",
    "                    him, to know him, to love him with all his strength. He calls together all men,\n",
    "                    scattered and divided by sin, into the unity of his family, the Church. To\n",
    "                    accomplish this, when the fullness of time had come, God sent his Son as\n",
    "                    Redeemer and Saviour. In his Son and through him, he invites men to become, in\n",
    "                    the Holy Spirit, his adopted children and thus heirs of his blessed life.\n",
    "                    2 So that this call should resound throughout the world, Christ sent forth the\n",
    "                    apostles he had chosen, commissioning them to proclaim the gospel: \"Go\n",
    "                    therefore and make disciples of all nations, baptizing them in the name of the\n",
    "                    Father and of the Son and of the Holy Spirit, teaching them to observe all that\n",
    "                    I have commanded you; and lo, I am with you always, to the close of the age.\"\n",
    "                    Strengthened by this mission, the apostles \"went forth and preached everywhere,\n",
    "                    while the Lord worked with them and confirmed the message by the signs that\n",
    "                    attended it.\"\n",
    "                    3 Those who with God's help have welcomed Christ's call and freely responded to\n",
    "                    it are urged on by love of Christ to proclaim the Good News everywhere in the\n",
    "                    world. This treasure, received from the apostles, has been faithfully guarded\n",
    "                    by their successors. All Christ's faithful are called to hand it on from\n",
    "                    generation to generation, by professing the faith, by living it in fraternal\n",
    "                    sharing, and by celebrating it in liturgy and prayer.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['god', 'infinitely', 'perfect', 'blessed', 'plan']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "catechism_text_tokens = filter_out_stopwords(tokenizer.tokenize(catechism_text.lower()))\n",
    "catechism_text_tokens = [w for w in catechism_text_tokens if w.isalpha()]\n",
    "catechism_text_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learned'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(extract_word_presence_absence_features(catechism_text_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
