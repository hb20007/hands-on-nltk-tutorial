{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Text Language by Counting Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Detecting Text Language With Python and NLTK](https://web.archive.org/web/20240613234104/https://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/) by Alejandro Nolla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yo, man! It's time to learn NLP. I'm for real, dawg!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will use `wordpunct_tokenize`, a regexp-based tokenizer that splits text on whitespace and punctuation (except for underscores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.tokenize import wordpunct_tokenize\n",
    "except ImportError:\n",
    "    print('[!] You need to install NLTK (https://www.nltk.org/).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yo',\n",
       " ',',\n",
       " 'man',\n",
       " '!',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'time',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'for',\n",
       " 'real',\n",
       " ',',\n",
       " 'dawg',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = wordpunct_tokenize(text)\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring NLTK’s stop words corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** are words that are filtered out before processing because they are mostly grammatical and not semantic in nature. E.g., search engines remove words like “want”.\n",
    "\n",
    "NLTK comes with a corpus of stop words in various languages called `stopwords`. We can learn more about it using the `readme()` function. It returns raw text, so, although it’s not mandatory, we use `print()` to process it as a string literal so that `\\n` escape sequences take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords Corpus\n",
      "\n",
      "This corpus contains lists of stop words for several languages\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.readme()[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files can be accessed via `fileids()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albanian',\n",
       " 'arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'belarusian',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus readers provide different methods for reading data from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n",
      "and\n",
      "any\n",
      "are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.raw('english')[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `sents()` and `paras()`, which return sentences and paragraphs, respectively. However, in our particular case, this would cause an error, because the stop words corpus reader is of type `WordListCorpusReader`, so there are no sentences or paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, corpus readers allow us to read the text of multiple files together. For example, we can count the total number of Norwegian and Swedish stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['norwegian', 'swedish']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though NLTK’s stop words corpus is intended to help us filter out stop words in the language we are working with, it is essentially a list of words in more than 30 languages. Therefore, we can use it to identify languages by checking the presence of these words. Of course, this is just an exercise and is not a serious method for classifying text by language.\n",
    "\n",
    "We will loop through the list of stop words in all languages and count how many stop words our text contains in each language. The text is then classified in the language in which it has the most stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST TEXT LANGUAGE \"SCORES\"\n",
      "Albanian: 3\n",
      "Bengali: 0\n",
      "English: 6\n"
     ]
    }
   ],
   "source": [
    "language_ratios = {}\n",
    "\n",
    "test_words = [word.lower() for word in test_tokens]\n",
    "test_words_set = set(test_words)\n",
    "\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_set = set(stopwords.words(language))\n",
    "    common_elements = test_words_set.intersection(stopwords_set)\n",
    "    language_ratios[language] = len(common_elements)\n",
    "\n",
    "print('TEST TEXT LANGUAGE \"SCORES\"')\n",
    "print('Albanian:', language_ratios['albanian'])\n",
    "print('Bengali:', language_ratios['bengali'])\n",
    "print('English:', language_ratios['english'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use Python’s `max()` function to find the language with the highest “score”. Since `language_ratios` is a dictionary, we use the `key` parameter to specify that the comparison should be performed based on the dictionary values rather than the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_rated_language = max(language_ratios, key=language_ratios.get)\n",
    "most_rated_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check which English stop words were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'for', 'i', 'it', 'm', 's', 'to'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words_set.intersection(set(stopwords.words(most_rated_language)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
