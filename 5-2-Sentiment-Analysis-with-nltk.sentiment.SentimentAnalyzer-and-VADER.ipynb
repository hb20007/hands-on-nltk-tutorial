{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with `nltk.sentiment.SentimentAnalyzer` and VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the `subjectivity` corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subjectivity` dataset contains 5000 objective and 5000 subjective sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obj', 'subj']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import subjectivity\n",
    "\n",
    "subjectivity.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'movie',\n",
       " 'begins',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'where',\n",
       " 'a',\n",
       " 'young',\n",
       " 'boy',\n",
       " 'named',\n",
       " 'sam',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'save',\n",
       " 'celebi',\n",
       " 'from',\n",
       " 'a',\n",
       " 'hunter',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity.sents(categories='obj')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart',\n",
       " 'and',\n",
       " 'alert',\n",
       " ',',\n",
       " 'thirteen',\n",
       " 'conversations',\n",
       " 'about',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'small',\n",
       " 'gem',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectivity.sents(categories='subj')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and testing a classifier with `SentimentAnalyzer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a classifier that classifies text as being either objective or subjective.\n",
    "\n",
    "We will start by importing `NaiveBayesClassifier`, `SentimentAnalyzer`, and two useful functions from `nltk.sentiment.util`: `mark_negation()` appends a `_NEG` suffix to words that appear in the scope between a negation and a punctuation mark, and `extract_unigram_feats()` populates a dictionary of word unigram features. Then, for this quick demo, we will create a small dataset of 100 objective and 100 subjective sentences from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import (mark_negation, extract_unigram_feats)\n",
    "\n",
    "N_INSTANCES = 100\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:N_INSTANCES]]\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:N_INSTANCES]]\n",
    "len(obj_docs), len(subj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'movie',\n",
       "  'begins',\n",
       "  'in',\n",
       "  'the',\n",
       "  'past',\n",
       "  'where',\n",
       "  'a',\n",
       "  'young',\n",
       "  'boy',\n",
       "  'named',\n",
       "  'sam',\n",
       "  'attempts',\n",
       "  'to',\n",
       "  'save',\n",
       "  'celebi',\n",
       "  'from',\n",
       "  'a',\n",
       "  'hunter',\n",
       "  '.'],\n",
       " 'obj')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_TEST_SPLIT = .8\n",
    "cutoff = int(len(obj_docs) * TRAIN_TEST_SPLIT)\n",
    "\n",
    "train_obj_docs = obj_docs[:cutoff]\n",
    "test_obj_docs = obj_docs[cutoff:]\n",
    "train_subj_docs = subj_docs[:cutoff]\n",
    "test_subj_docs = subj_docs[cutoff:]\n",
    "\n",
    "training_docs = train_obj_docs + train_subj_docs\n",
    "testing_docs = test_obj_docs + test_subj_docs\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "all_words_with_negation = sentiment_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "all_words_with_negation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_with_negation, min_freq=4)\n",
    "len(unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOME EXAMPLE UNIGRAM FEATURES FOR THE FIRST OBJECTIVE SENTENCE\n",
      "contains(.): True\n",
      "contains(it): False\n",
      "contains(the): True\n",
      "contains(love): False\n",
      "contains(but_NEG): False\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "\n",
    "training_set = sentiment_analyzer.apply_features(training_docs)\n",
    "test_set = sentiment_analyzer.apply_features(testing_docs)\n",
    "\n",
    "print('SOME EXAMPLE UNIGRAM FEATURES FOR THE FIRST OBJECTIVE SENTENCE')\n",
    "print('contains(.):', training_set[0][0]['contains(.)'])\n",
    "print('contains(it):', training_set[0][0]['contains(it)'])\n",
    "print('contains(the):', training_set[0][0]['contains(the)'])\n",
    "print('contains(love):', training_set[0][0]['contains(love)'])\n",
    "print('contains(but_NEG):', training_set[0][0]['contains(but_NEG)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(sentiment_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment analysis with `nltk.sentiment.vader.SentimentIntensityAnalyzer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VADER** is a parsimonious rule-based model for the sentiment analysis of social media text. It was first developed as a [separate tool](https://github.com/cjhutto/vaderSentiment) and later integrated into NLTK.\n",
    "\n",
    "VADER is based on a **sentiment lexicon** with sentiment ratings from 10 independent human raters for over 7,500 tokens on a scale from $ [–4] $ (extremely negative) to $ [4] $ (extremely positive). E.g., `okay` has a positive valence of $ 0.9 $, `good` is $ 1.9 $, and `great` is $ 3.1 $, whereas `sucks`/`sux` is $ –1.5 $, `:(` is $ –2.2 $, and `horrible` is $ –2.5 $.\n",
    "\n",
    "On top of the sentiment lexicon, VADER employs several **rule-based enhancements** like word-order sensitivity, degree modifiers, word-shape amplifiers, punctuation amplifiers, negation polarity switches, and contrastive conjunction sensitivity.\n",
    "\n",
    "When tested on a benchmark set of tweets, VADER’s sentiment assessment outperformed individual human raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a piece of shit, and I will step on you.\n",
      "compound: -0.5574, neg: 0.286, neu: 0.714, pos: 0.0, \n",
      "\n",
      "THIS SUCKS!\n",
      "compound: -0.4199, neg: 0.736, neu: 0.264, pos: 0.0, \n",
      "\n",
      "This kinda sux...\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "\n",
      "You're good, man!\n",
      "compound: 0.4926, neg: 0.0, neu: 0.385, pos: 0.615, \n",
      "\n",
      "DAMN, YOU ARE THE BEST! VERY FUNNY!!!\n",
      "compound: 0.7821, neg: 0.177, neu: 0.262, pos: 0.561, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentences = [\n",
    "    \"You are a piece of shit, and I will step on you.\",\n",
    "    \"THIS SUCKS!\",\n",
    "    \"This kinda sux...\",\n",
    "    \"You're good, man!\",\n",
    "    \"DAMN, YOU ARE THE BEST! VERY FUNNY!!!\"\n",
    "            ]\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    sentiment_strength = sid.polarity_scores(sentence)\n",
    "    for k in sorted(sentiment_strength):\n",
    "        print('{0}: {1}, '.format(k, sentiment_strength[k]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, `compound` represents the aggregated, final score. It is computed by summing each word’s valence score and then normalizing to the range $ [-1, 1] $. The individual `neg`, `neu`, and `pos` scores are ratios for proportions of text that fall under each category (so they should add up to $ 1 $) and are useful for digging deeper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
